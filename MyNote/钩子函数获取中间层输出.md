# 钩子函数获取中间层输出

本科毕业设计具体应用：获取模型最后K层输出

```python3
class Data2vec(nn.Module):

def __init__(self,
            in_features: int,
            seq_len: int,
            dim: int = 768, 
            nhead: int = 4, # 自注意力头数目
            dropout: float = 0.1,
            n_encoders: int = 8) -> None:
    super().__init__()
    self.in_features = in_features
    self.seq_len = seq_len
    self.dim = dim
    self.dropout = dropout
    self.nhead = nhead
    self.n_encoders = n_encoders

    # transformer encoder 单元

​    el = nn.TransformerEncoderLayer(d_model=dim,
​                                    nhead=nhead,
​                                    dim_feedforward=4*dim,
​                                    dropout=dropout)
​    self.encoder = nn.TransformerEncoder(el, n_encoders)
​    self.patch_linear = nn.Linear(self.in_features, dim)
​    self.pos_embedding = PositionalEmbedding1D(seq_len, dim)
​    
​    self.regressor = nn.Sequential(
​        nn.Linear(dim, dim),
​        nn.GELU(),
​        nn.Linear(dim, dim),
​    )
​    self.hs: List[torch.Tensor] = []
​    for l in self.encoder.layers:
​        l.dropout2.register_forward_hook(self._register_hs_hook)

def _register_hs_hook(self, layer: nn.Module, inputs: torch.Tensor,
                    outputs: torch.Tensor) -> None:
    self.hs.append(outputs)

def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    self.hs.clear()
    x = self.patch_linear(x)
    x = self.pos_embedding(x)
    x = self.encoder(x)

​    '''输出shape [2, 196, 768]'''

            # self-distill 装置 student mode 回归 teacher mode, 加上回归层
            # 拼接 tensor (block_num, ...)

​    return self.regressor(x), torch.stack([h.clone() for h in self.hs])
```

> 关于clone,detach,copy区别:
>
> 1. clone追溯梯度，但不共享内存
> 2. detach共享内存，但是不追溯梯度
> 3. new_tensor(data, dtype=None, device=None, requires_grad=False)  两者都不，也相当于copy_()
>
> 参考链接: https://blog.csdn.net/dujuancao11/article/details/121563226

参考文章实现：

1.

```python3
import torch
import torch.nn as nn
model = nn.Sequential(
            nn.Conv2d(3, 9, 1, 1, 0, bias=False),
            nn.BatchNorm2d(9),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1)),
        )

# 假如想要获得ReLu的输出
x = torch.rand([2, 3, 224, 224])
for i in range(len(model)):
    x = model[i](x)
    if i == 2:
        ReLu_out = x
print('ReLu_out.shape：\n\t',ReLu_out.shape)
print('x.shape：\n\t',x.shape)
```

2.

```python
class TestForHook(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear_1 = nn.Linear(in_features=2, out_features=2)
        self.linear_2 = nn.Linear(in_features=2, out_features=1)
        self.relu = nn.ReLU()
        self.relu6 = nn.ReLU6()
        self.initialize()

    def forward(self, x):
        linear_1 = self.linear_1(x)
        linear_2 = self.linear_2(linear_1)
        relu = self.relu(linear_2)
        relu_6 = self.relu6(relu)
        layers_in = (x, linear_1, linear_2)
        layers_out = (linear_1, linear_2, relu)
        return relu_6, layers_in, layers_out

features_in_hook = []
features_out_hook = []

def hook(module, fea_in, fea_out):
    features_in_hook.append(fea_in)
    features_out_hook.append(fea_out)
    return None

net = TestForHook()

"""
# 第一种写法，按照类型勾，但如果有重复类型的layer比较复杂
net_chilren = net.children()
for child in net_chilren:
    if not isinstance(child, nn.ReLU6):
        child.register_forward_hook(hook=hook)
"""

"""
推荐下面我改的这种写法，因为我自己的网络中，在Sequential中有很多层，
这种方式可以直接先print(net)一下，找出自己所需要那个layer的名称，按名称勾出来
"""
layer_name = 'relu_6'
for (name, module) in net.named_modules():
    if name == layer_name:
        module.register_forward_hook(hook=hook)

print(features_in_hook)  # 勾的是指定层的输入
print(features_out_hook)  # 勾的是指定层的输出
```

相关链接: https://blog.csdn.net/Dr_maker/article/details/123318086